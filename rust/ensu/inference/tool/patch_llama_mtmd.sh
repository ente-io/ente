#!/usr/bin/env bash
set -euo pipefail

ROOT="$(cd "$(dirname "$0")/.." && pwd)"
CRATE_DIR="$(
  python3 - <<PY
import json
import os
import subprocess
import sys
from pathlib import Path

root = os.path.abspath("${ROOT}")
try:
    meta = subprocess.check_output(["cargo", "metadata", "--format-version", "1"], cwd=root)
except Exception:
    sys.exit(1)

data = json.loads(meta)
for pkg in data.get("packages", []):
    if pkg.get("name") == "llama-cpp-sys-2":
        print(Path(pkg["manifest_path"]).parent)
        sys.exit(0)

sys.exit(1)
PY
)" || true

if [[ -z "$CRATE_DIR" ]]; then
  VERSION=$(grep -E "llama-cpp-2" "$ROOT/Cargo.toml" | head -n 1 | grep -Eo 'version *= *"[^"]+"|"[0-9]+\.[0-9]+\.[0-9]+"' | head -n 1 | sed -E 's/.*"([^"]+)".*/\1/' | sed -E 's/^=//')
  CRATE_DIR=$(ls -d "$HOME/.cargo/registry/src/"*/llama-cpp-sys-2-$VERSION 2>/dev/null | head -n 1 || true)
fi

if [[ -z "$CRATE_DIR" ]]; then
  echo "llama-cpp-sys-2 not found in cargo metadata or registry. Run 'cargo fetch' in $ROOT and try again." >&2
  exit 1
fi

TOOLS_DIR="$CRATE_DIR/llama.cpp/tools"
MTMD_DIR="$TOOLS_DIR/mtmd"
CMAKE_FILE="$TOOLS_DIR/CMakeLists.txt"
VENDOR_DIR="$CRATE_DIR/llama.cpp/vendor"

if [[ ! -d "$MTMD_DIR" ]]; then
  echo "mtmd tools directory not found at $MTMD_DIR" >&2
  exit 0
fi

if [[ ! -f "$CMAKE_FILE" ]]; then
  cat > "$CMAKE_FILE" <<'EOF'
# Auto-generated by inference_rs_dart to enable mtmd sources in llama.cpp builds.
if (LLAMA_BUILD_TOOLS)
  if (EXISTS "${CMAKE_CURRENT_LIST_DIR}/mtmd")
    set(LLAMA_MTMD_SOURCES
        ${CMAKE_CURRENT_LIST_DIR}/mtmd/clip.cpp
        ${CMAKE_CURRENT_LIST_DIR}/mtmd/mtmd.cpp
        ${CMAKE_CURRENT_LIST_DIR}/mtmd/mtmd-audio.cpp
        ${CMAKE_CURRENT_LIST_DIR}/mtmd/mtmd-helper.cpp
    )
    target_sources(llama PRIVATE ${LLAMA_MTMD_SOURCES})
    target_include_directories(llama PRIVATE
      ${CMAKE_CURRENT_LIST_DIR}/mtmd
      ${CMAKE_CURRENT_LIST_DIR}/../vendor
    )
  endif()
endif()
EOF
  echo "Patched $CMAKE_FILE for mtmd."
fi

CLIP_CPP="$MTMD_DIR/clip.cpp"
if [[ -f "$CLIP_CPP" ]]; then
  python3 - <<PY
from pathlib import Path
import sys

path = Path("$CLIP_CPP")
text = path.read_text(encoding="utf-8", errors="ignore")

replacements = {
    """            case PROJECTOR_TYPE_LFM2:\n            case PROJECTOR_TYPE_KIMIVL:\n                {\n                    model.mm_input_norm_w = get_tensor(TN_MM_INP_NORM);\n                    model.mm_input_norm_b = get_tensor(TN_MM_INP_NORM_B);\n                    model.mm_1_w = get_tensor(string_format(TN_LLAVA_PROJ, 1, \"weight\"));\n                    model.mm_1_b = get_tensor(string_format(TN_LLAVA_PROJ, 1, \"bias\"));\n                    model.mm_2_w = get_tensor(string_format(TN_LLAVA_PROJ, 2, \"weight\"));\n                    model.mm_2_b = get_tensor(string_format(TN_LLAVA_PROJ, 2, \"bias\"));\n                } break;\n""": """            case PROJECTOR_TYPE_LFM2:\n            case PROJECTOR_TYPE_KIMIVL:\n                {\n                    model.mm_input_norm_w = get_tensor(TN_MM_INP_NORM, false);\n                    model.mm_input_norm_b = get_tensor(TN_MM_INP_NORM_B, false);\n                    model.mm_1_w = get_tensor(string_format(TN_LLAVA_PROJ, 1, \"weight\"));\n                    model.mm_1_b = get_tensor(string_format(TN_LLAVA_PROJ, 1, \"bias\"));\n                    model.mm_2_w = get_tensor(string_format(TN_LLAVA_PROJ, 2, \"weight\"));\n                    model.mm_2_b = get_tensor(string_format(TN_LLAVA_PROJ, 2, \"bias\"));\n                } break;\n""",
    """            cur = ggml_norm(ctx0, cur, 1e-5); // default nn.LayerNorm\n            cur = ggml_mul(ctx0, cur, model.mm_input_norm_w);\n            cur = ggml_add(ctx0, cur, model.mm_input_norm_b);\n\n            cur = ggml_mul_mat(ctx0, model.mm_1_w, cur);\n""": """            cur = ggml_norm(ctx0, cur, 1e-5); // default nn.LayerNorm\n            if (model.mm_input_norm_w && model.mm_input_norm_b) {\n                cur = ggml_mul(ctx0, cur, model.mm_input_norm_w);\n                cur = ggml_add(ctx0, cur, model.mm_input_norm_b);\n            }\n\n            cur = ggml_mul_mat(ctx0, model.mm_1_w, cur);\n""",
    """            cur = ggml_norm(ctx0, cur, 1e-5); // default nn.LayerNorm\n            cur = ggml_mul(ctx0, cur, model.mm_input_norm_w);\n            cur = ggml_add(ctx0, cur, model.mm_input_norm_b);\n            cur = ggml_view_2d(ctx0, cur,\n""": """            cur = ggml_norm(ctx0, cur, 1e-5); // default nn.LayerNorm\n            if (model.mm_input_norm_w && model.mm_input_norm_b) {\n                cur = ggml_mul(ctx0, cur, model.mm_input_norm_w);\n                cur = ggml_add(ctx0, cur, model.mm_input_norm_b);\n            }\n            cur = ggml_view_2d(ctx0, cur,\n""",
    """            cur = ggml_mul(ctx0, ggml_rms_norm(ctx0, cur, eps), model.mm_input_norm_w);\n""": """            cur = ggml_rms_norm(ctx0, cur, eps);\n            if (model.mm_input_norm_w) {\n                cur = ggml_mul(ctx0, cur, model.mm_input_norm_w);\n            }\n""",
}

updated = text
applied = []
missing = []

for old, new in replacements.items():
    if old in text:
        updated = updated.replace(old, new)
        applied.append(old)
    elif new in text:
        continue
    else:
        missing.append(old)

if updated != text:
    path.write_text(updated, encoding="utf-8")

if missing:
    sys.stderr.write("mtmd clip.cpp patch patterns not found; llama.cpp may have changed.\n")
    sys.stderr.write(f"File: {path}\n")
    sys.exit(1)

marker = Path("$CRATE_DIR") / "llama.cpp" / "CMakeMtmdPatch.txt"
needs_rebuild = applied or not marker.exists()
if needs_rebuild:
    marker.write_text("# mtmd patch applied\n")
    cmake_root = Path("$CRATE_DIR") / "llama.cpp" / "CMakeLists.txt"
    if cmake_root.exists():
        cmake_root.touch()

if applied:
    print(f"Patched mtmd clip.cpp ({len(applied)} replacements).")
else:
    print("mtmd clip.cpp already patched.")
PY
fi
